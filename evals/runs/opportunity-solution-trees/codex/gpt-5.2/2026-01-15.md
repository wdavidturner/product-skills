# Skill Eval Feedback

skill: opportunity-solution-trees
agent: codex
model: gpt-5.2
date: 2026-01-15
prompt-set: evals/prompts/skill-prompts.yaml

## Prompt Results

| Prompt ID | Triggered? | Helpful? (1-5) | Too Much Context? | Missing Context? | Notes |
|-----------|------------|----------------|-------------------|------------------|-------|
| minimal |  |  |  |  |  |
| context |  |  |  |  |  |

## Overall

- Trigger quality:
- Usefulness:
- Over/under-trigger risk:
- Suggested description tweaks:
- Suggested SKILL.md tweaks:

## Raw Output

### minimal

Prompt:

```
How do Opportunity Solution Trees work? Walk me through building one.
```

Response:

```
Opportunity Solution Trees (OSTs) are a way to connect what you’re trying to achieve (an outcome) to what you might build (solutions) through the customer problems/needs you’ve learned (opportunities), with explicit validation (experiments). Think: **Outcome → Opportunities → Solutions → Experiments**.

**How they work**
- **Outcome (top/root):** A measurable result you want (not a feature), with a time horizon and metric.
- **Opportunities (middle branches):** Customer needs, pain points, desires, frictions, and jobs-to-be-done you’ve discovered via research/data.
- **Solutions (lower branches):** Possible product/process changes that could address a specific opportunity.
- **Experiments (leaves):** Fast ways to test assumptions before committing (prototype test, concierge, fake door, pricing test, etc.).

**Walkthrough: building one**
1. **Pick a single outcome.** Make it metric-based and specific.  
   Example: “Increase weekly activation from 22% → 30% by end of Q2.”
2. **Define “activation” precisely.** Write the behavioral definition and current baseline so everyone shares the same target.
3. **Generate opportunities from evidence.** Pull from interviews, support tickets, session replays, funnel drop-offs, churn reasons, sales calls. Phrase each as a customer statement or observable struggle.  
   Good: “New users don’t know what to do after importing data.”  
   Avoid: “Add onboarding checklist” (that’s already a solution).
4. **Group and label opportunity clusters.** You’ll typically see 5–15 clusters (e.g., “confusing setup,” “unclear value,” “missing integrations”).
5. **Prioritize opportunities (not solutions).** Use a simple score like:
   - **Importance** to the outcome (how strongly it blocks/drives the metric)
   - **Frequency/reach** (how many users it affects)
   - **Confidence** (quality of evidence)
   Pick 1–2 top opportunities to focus on.
6. **Ideate multiple solutions per top opportunity.** Force breadth: aim for 5–10 solution ideas before converging. Mix product, UX, pricing, ops, and messaging options.
7. **Select solutions by assumptions.** For each candidate solution, write the key assumptions (desirability, usability, feasibility, viability) and choose the ones with the best **expected impact** and **testability**.
8. **Attach experiments to de-risk.** For each selected solution, define:
   - hypothesis (If we…, then…, because…)
   - success metric + threshold
   - test method + segment
   - timeframe
9. **Run tests, learn, and prune.** Promote branches that show evidence; delete or revise branches that don’t. The tree is a living artifact, not a one-time doc.

**Common mistakes**
- Starting with a solution you already want and “backfilling” opportunities.
- Using vague outcomes (“improve engagement”) or mixing multiple outcomes in one tree.
- Listing opportunities that are really internal tasks (“refactor onboarding flow”).
- Skipping experiments and going straight to build.

If you tell me your product and the outcome you’re targeting (metric + timeframe), I can help you draft the first version of an OST with opportunity prompts and a prioritization pass.

[stderr]
OpenAI Codex v0.80.0 (research preview)
--------
workdir: /Users/davidturner/Code/Playground/ProductSkills
model: gpt-5.2
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019bc21b-8939-7150-b17f-f73d4c2bc4e7
--------
user
How do Opportunity Solution Trees work? Walk me through building one.
mcp startup: no servers

thinking
**Explaining a structured approach**

The user is asking for a concise, structured explanation without code changes. I need to provide a clear flow of steps while keeping it under ten lines. Since they requested, “Walk me through,” I should incorporate headers, perhaps referencing a framework by Teresa Torres. 

I’ll define concepts like outcome metrics, opportunities, solutions, experiments, and outline the necessary steps, including defining outcome metrics, identifying and prioritizing opportunities, ideating solutions, and iterating. I should also ask for context regarding the product.
codex
Opportunity Solution Trees (OSTs) are a way to connect what you’re trying to achieve (an outcome) to what you might build (solutions) through the customer problems/needs you’ve learned (opportunities), with explicit validation (experiments). Think: **Outcome → Opportunities → Solutions → Experiments**.

**How they work**
- **Outcome (top/root):** A measurable result you want (not a feature), with a time horizon and metric.
- **Opportunities (middle branches):** Customer needs, pain points, desires, frictions, and jobs-to-be-done you’ve discovered via research/data.
- **Solutions (lower branches):** Possible product/process changes that could address a specific opportunity.
- **Experiments (leaves):** Fast ways to test assumptions before committing (prototype test, concierge, fake door, pricing test, etc.).

**Walkthrough: building one**
1. **Pick a single outcome.** Make it metric-based and specific.  
   Example: “Increase weekly activation from 22% → 30% by end of Q2.”
2. **Define “activation” precisely.** Write the behavioral definition and current baseline so everyone shares the same target.
3. **Generate opportunities from evidence.** Pull from interviews, support tickets, session replays, funnel drop-offs, churn reasons, sales calls. Phrase each as a customer statement or observable struggle.  
   Good: “New users don’t know what to do after importing data.”  
   Avoid: “Add onboarding checklist” (that’s already a solution).
4. **Group and label opportunity clusters.** You’ll typically see 5–15 clusters (e.g., “confusing setup,” “unclear value,” “missing integrations”).
5. **Prioritize opportunities (not solutions).** Use a simple score like:
   - **Importance** to the outcome (how strongly it blocks/drives the metric)
   - **Frequency/reach** (how many users it affects)
   - **Confidence** (quality of evidence)
   Pick 1–2 top opportunities to focus on.
6. **Ideate multiple solutions per top opportunity.** Force breadth: aim for 5–10 solution ideas before converging. Mix product, UX, pricing, ops, and messaging options.
7. **Select solutions by assumptions.** For each candidate solution, write the key assumptions (desirability, usability, feasibility, viability) and choose the ones with the best **expected impact** and **testability**.
8. **Attach experiments to de-risk.** For each selected solution, define:
   - hypothesis (If we…, then…, because…)
   - success metric + threshold
   - test method + segment
   - timeframe
9. **Run tests, learn, and prune.** Promote branches that show evidence; delete or revise branches that don’t. The tree is a living artifact, not a one-time doc.

**Common mistakes**
- Starting with a solution you already want and “backfilling” opportunities.
- Using vague outcomes (“improve engagement”) or mixing multiple outcomes in one tree.
- Listing opportunities that are really internal tasks (“refactor onboarding flow”).
- Skipping experiments and going straight to build.

If you tell me your product and the outcome you’re targeting (metric + timeframe), I can help you draft the first version of an OST with opportunity prompts and a prioritization pass.
tokens used
1,442
```


### scenario

Prompt:

```
We have a clear outcome goal but 50 different ideas for how to achieve it. The team is paralyzed by options. How do we prioritize discovery work?
```

Response:

```
Prioritize discovery by maximizing “value of information” per unit time: test the assumptions that could most quickly change what you build (or whether you build anything).

- **Make the outcome concrete**: one target metric + timeframe + audience (e.g., “Increase activation from 18%→25% for new SMB users in 8 weeks”).
- **Turn 50 ideas into an assumption map**: for each idea, write (a) user, (b) job/problem, (c) promised value, (d) mechanism, (e) proof needed. If you can’t state these in 2–3 sentences, it’s not ready for discovery.
- **Score ideas by Risk × Impact (not effort)**:
  - **Impact**: if true, how much does it move the outcome?
  - **Uncertainty**: how likely is it false / unknown?
  - **Decision leverage**: will the result change what you do next (yes/no)?
  - Prioritize the top items where **uncertainty and impact are both high** and the result will clearly drive a decision.
- **Define the “killer assumption” for each top idea**: the one assumption that, if wrong, makes the idea not worth doing (e.g., “Users will switch behavior weekly,” “We can reach them in-product,” “They’ll pay $X,” “It reduces time-to-value by Y”).
- **Pick the cheapest test that can falsify it** (in increasing cost):
  - Existing data query → 5-user interviews → prototype click-test → concierge/MVP → pricing test → build.
  - Aim for **1–2 days per test**, not multi-week research projects.
- **Run in small batches**: choose **3–5 discovery bets** max for a 1–2 week cycle; everything else goes to an “icebox” with its assumption statement (so it’s not forgotten, just not active).
- **Use explicit decision rules**: before testing, write what “pass / fail / ambiguous” means and what you’ll do next. This prevents endless debate.
- **Review cadence**: weekly 30-minute “evidence review” where the only outputs are (1) what we learned, (2) which ideas got promoted/demoted/killed, (3) next tests.

If you share your outcome metric and the 50 ideas in rough categories (or top 10), I can help you build a one-page scoring grid and identify the likely highest-leverage killer assumptions to test first.

[stderr]
OpenAI Codex v0.80.0 (research preview)
--------
workdir: /Users/davidturner/Code/Playground/ProductSkills
model: gpt-5.2
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019bc21b-d50d-74c1-b9f6-21ef2b87d4be
--------
user
We have a clear outcome goal but 50 different ideas for how to achieve it. The team is paralyzed by options. How do we prioritize discovery work?
mcp startup: no servers

thinking
**Providing a framework for product questions**

The user is asking a general question about a product or process, and I don’t need any tools for this. I'll outline a concise framework that includes identifying the riskiest assumptions and using an opportunity solution tree, along with impact/effort considerations. 

I’ll suggest defining an outcome metric, mapping ideas, and scoring by uncertainty and impact. This leads to choosing the top 3-5 tests to conduct with the cheapest method.

I should clarify decision rules and meeting structures, and I can ask for context if needed, while keeping it minimal and direct.
codex
Prioritize discovery by maximizing “value of information” per unit time: test the assumptions that could most quickly change what you build (or whether you build anything).

- **Make the outcome concrete**: one target metric + timeframe + audience (e.g., “Increase activation from 18%→25% for new SMB users in 8 weeks”).
- **Turn 50 ideas into an assumption map**: for each idea, write (a) user, (b) job/problem, (c) promised value, (d) mechanism, (e) proof needed. If you can’t state these in 2–3 sentences, it’s not ready for discovery.
- **Score ideas by Risk × Impact (not effort)**:
  - **Impact**: if true, how much does it move the outcome?
  - **Uncertainty**: how likely is it false / unknown?
  - **Decision leverage**: will the result change what you do next (yes/no)?
  - Prioritize the top items where **uncertainty and impact are both high** and the result will clearly drive a decision.
- **Define the “killer assumption” for each top idea**: the one assumption that, if wrong, makes the idea not worth doing (e.g., “Users will switch behavior weekly,” “We can reach them in-product,” “They’ll pay $X,” “It reduces time-to-value by Y”).
- **Pick the cheapest test that can falsify it** (in increasing cost):
  - Existing data query → 5-user interviews → prototype click-test → concierge/MVP → pricing test → build.
  - Aim for **1–2 days per test**, not multi-week research projects.
- **Run in small batches**: choose **3–5 discovery bets** max for a 1–2 week cycle; everything else goes to an “icebox” with its assumption statement (so it’s not forgotten, just not active).
- **Use explicit decision rules**: before testing, write what “pass / fail / ambiguous” means and what you’ll do next. This prevents endless debate.
- **Review cadence**: weekly 30-minute “evidence review” where the only outputs are (1) what we learned, (2) which ideas got promoted/demoted/killed, (3) next tests.

If you share your outcome metric and the 50 ideas in rough categories (or top 10), I can help you build a one-page scoring grid and identify the likely highest-leverage killer assumptions to test first.
tokens used
1,519
```


### application

Prompt:

```
Help me build an Opportunity Solution Tree. Our outcome is 'increase trial-to-paid conversion by 20%'. What opportunities should we explore, and how do I structure the tree?
```

Response:

```
**How to structure the OST**
- **Outcome (top):** “Increase trial-to-paid conversion by 20%” (define baseline, timeframe, and what counts as “paid”).
- **Opportunities (next level):** customer problems / needs / frictions that, if improved, plausibly move the outcome.
- **Solutions (under each opportunity):** candidate product/UX/pricing/support changes that might address the opportunity.
- **Experiments (lowest level):** tests to validate solution impact (A/B, usability, concierge tests, pricing research, etc.).

**Opportunities to explore (good starting branches)**
- **Activation & first value:** users don’t reach the “aha” moment fast enough during trial.
- **Time-to-value blockers:** setup/integration/data import is too hard or too slow.
- **Value comprehension:** users do get value, but don’t understand ROI/differentiation well enough to pay.
- **Habit formation & engagement:** users try it once but don’t build a repeat workflow before trial ends.
- **Paywall & plan fit:** packaging limits don’t align with what trial users need to succeed; plan selection is confusing.
- **Pricing & willingness to pay:** price is misaligned with perceived value, segment, or competitor anchors.
- **Purchase friction:** checkout, invoicing, permissions, procurement/security reviews slow or stop purchase.
- **Trust & risk:** unclear reliability, security, data handling, or support expectations.
- **Lifecycle & timing:** prompts/offers happen at the wrong moments; trial expiry surprises; poor nudges at key milestones.
- **Right users in the trial:** acquisition brings low-intent or wrong-role users who can’t buy.

**Example tree (template you can copy)**
- **Outcome:** Increase trial-to-paid conversion by 20%
  - **Opportunity:** Reduce time to first value
    - **Solutions:** guided setup checklist; import wizard; sample project/data; “done-for-you” setup
    - **Experiments:** instrument TTFV; usability test setup; A/B wizard vs baseline
  - **Opportunity:** Improve “why pay” clarity at moment of value
    - **Solutions:** in-product ROI summary; comparison to alternatives; personalized outcomes dashboard
    - **Experiments:** A/B value messaging; measure paywall views→upgrade; interview recent converters/non-converters
  - **Opportunity:** Increase engagement in week 1
    - **Solutions:** role-based onboarding; templates; reminders tied to user goals; collaboration invites
    - **Experiments:** cohort analysis; lifecycle messaging test; activation milestone nudges
  - **Opportunity:** Align paywall/limits with success path
    - **Solutions:** move a key feature earlier; usage-based limit change; clearer upgrade paths; plan recommender
    - **Experiments:** price/packaging research; A/B limit positioning; monitor “blocked event”→upgrade
  - **Opportunity:** Reduce purchase friction for buyers
    - **Solutions:** self-serve invoices; security docs page; SSO/trial admin controls; procurement workflow
    - **Experiments:** track checkout drop-off; survey “why didn’t you upgrade”; sales-assisted pilot for target accounts

**How to pick the “right” opportunities**
- Anchor each opportunity to **evidence** (funnel drop-offs, “blocked” events, surveys, support tickets, session replays, interviews).
- Write opportunities as **problem statements** (not solutions), e.g., “Admins can’t evaluate without SSO” vs “Add SSO.”
- Prioritize by **impact × confidence × effort** and by where the funnel is leaking most (activation, engagement, paywall, checkout).

If you tell me your product type (B2B/B2C), trial length, and the biggest drop-off step (activation vs paywall vs checkout), I can propose a tighter set of 5–7 opportunities and a first-pass tree tailored to your funnel.

[stderr]
OpenAI Codex v0.80.0 (research preview)
--------
workdir: /Users/davidturner/Code/Playground/ProductSkills
model: gpt-5.2
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019bc21c-119f-7cb2-abf0-71fc044e7257
--------
user
Help me build an Opportunity Solution Tree. Our outcome is 'increase trial-to-paid conversion by 20%'. What opportunities should we explore, and how do I structure the tree?
mcp startup: no servers
codex
**How to structure the OST**
- **Outcome (top):** “Increase trial-to-paid conversion by 20%” (define baseline, timeframe, and what counts as “paid”).
- **Opportunities (next level):** customer problems / needs / frictions that, if improved, plausibly move the outcome.
- **Solutions (under each opportunity):** candidate product/UX/pricing/support changes that might address the opportunity.
- **Experiments (lowest level):** tests to validate solution impact (A/B, usability, concierge tests, pricing research, etc.).

**Opportunities to explore (good starting branches)**
- **Activation & first value:** users don’t reach the “aha” moment fast enough during trial.
- **Time-to-value blockers:** setup/integration/data import is too hard or too slow.
- **Value comprehension:** users do get value, but don’t understand ROI/differentiation well enough to pay.
- **Habit formation & engagement:** users try it once but don’t build a repeat workflow before trial ends.
- **Paywall & plan fit:** packaging limits don’t align with what trial users need to succeed; plan selection is confusing.
- **Pricing & willingness to pay:** price is misaligned with perceived value, segment, or competitor anchors.
- **Purchase friction:** checkout, invoicing, permissions, procurement/security reviews slow or stop purchase.
- **Trust & risk:** unclear reliability, security, data handling, or support expectations.
- **Lifecycle & timing:** prompts/offers happen at the wrong moments; trial expiry surprises; poor nudges at key milestones.
- **Right users in the trial:** acquisition brings low-intent or wrong-role users who can’t buy.

**Example tree (template you can copy)**
- **Outcome:** Increase trial-to-paid conversion by 20%
  - **Opportunity:** Reduce time to first value
    - **Solutions:** guided setup checklist; import wizard; sample project/data; “done-for-you” setup
    - **Experiments:** instrument TTFV; usability test setup; A/B wizard vs baseline
  - **Opportunity:** Improve “why pay” clarity at moment of value
    - **Solutions:** in-product ROI summary; comparison to alternatives; personalized outcomes dashboard
    - **Experiments:** A/B value messaging; measure paywall views→upgrade; interview recent converters/non-converters
  - **Opportunity:** Increase engagement in week 1
    - **Solutions:** role-based onboarding; templates; reminders tied to user goals; collaboration invites
    - **Experiments:** cohort analysis; lifecycle messaging test; activation milestone nudges
  - **Opportunity:** Align paywall/limits with success path
    - **Solutions:** move a key feature earlier; usage-based limit change; clearer upgrade paths; plan recommender
    - **Experiments:** price/packaging research; A/B limit positioning; monitor “blocked event”→upgrade
  - **Opportunity:** Reduce purchase friction for buyers
    - **Solutions:** self-serve invoices; security docs page; SSO/trial admin controls; procurement workflow
    - **Experiments:** track checkout drop-off; survey “why didn’t you upgrade”; sales-assisted pilot for target accounts

**How to pick the “right” opportunities**
- Anchor each opportunity to **evidence** (funnel drop-offs, “blocked” events, surveys, support tickets, session replays, interviews).
- Write opportunities as **problem statements** (not solutions), e.g., “Admins can’t evaluate without SSO” vs “Add SSO.”
- Prioritize by **impact × confidence × effort** and by where the funnel is leaking most (activation, engagement, paywall, checkout).

If you tell me your product type (B2B/B2C), trial length, and the biggest drop-off step (activation vs paywall vs checkout), I can propose a tighter set of 5–7 opportunities and a first-pass tree tailored to your funnel.
tokens used
1,879
```


