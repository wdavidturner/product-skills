# Skill Eval Feedback

skill: hierarchy-of-engagement
agent: codex
model: gpt-5.2
date: 2026-01-15
prompt-set: evals/prompts/skill-prompts.yaml

## Prompt Results

| Prompt ID | Triggered? | Helpful? (1-5) | Too Much Context? | Missing Context? | Notes |
|-----------|------------|----------------|-------------------|------------------|-------|
| minimal |  |  |  |  |  |
| context |  |  |  |  |  |

## Overall

- Trigger quality:
- Usefulness:
- Over/under-trigger risk:
- Suggested description tweaks:
- Suggested SKILL.md tweaks:

## Raw Output

### minimal

Prompt:

```
How do I identify and measure our product's core action?
```

Response:

```
Identify your product’s core action by working backward from the value a user comes for, then define the smallest observable behavior that reliably delivers (or strongly predicts) that value.

- **Start with the value exchange:** “A user gets X value when they do Y.” Write 3–5 candidate Y’s and pick the one that best represents repeatable value (not setup, not browsing).
- **Make it atomic + instrumentable:** One action (or a tight sequence) you can log as an event, with clear start/end (e.g., “sent first message,” “created and shared a doc,” “completed first workout”).
- **Validate it predicts retention:** Check which candidate action correlates most with D7/D30 retention or repeat usage (simple cohort comparison is enough to start).
- **Define the metric:** Usually a **core-action rate** + **frequency**:
  - Rate: `% of new users who do core action within T (e.g., 7 days)`  
  - Frequency: `core actions per active user per week` (or per account)
- **Add funnel + time-to-value:** Track `time to first core action` and the steps immediately preceding it (activation funnel).
- **Add guardrails:** Quality and sustainability metrics (e.g., cancellations, spam/abuse, refunds, support tickets) so you don’t “win” by harming users.
- **Segment + cohort:** Break out by acquisition channel, persona/use case, platform, geography, and new vs returning; monitor trends over time, not just totals.

If you tell me your product type and what users “hire” it for, I can help you phrase a crisp core-action definition and a starter measurement dashboard.
```


### scenario

Prompt:

```
We have lots of sign-ups but retention is terrible. Users try the product once and never come back. How do we figure out what's broken?
```

Response:

```
Start by treating retention like a diagnosis problem: measure where people *fail to reach/feel value*, then confirm whether it’s *product/UX*, *audience mismatch*, or *missing reason to return*.

**1) Quantify the problem (cohorts, not averages)**
- Build weekly signup cohorts and chart `D1/D7/D30` retention and “% who ever return”.
- Segment by acquisition channel, persona/use case, platform, geography, pricing plan, and “first-session intent” (what they tried to do).
- Identify *one or two* cohorts/segments that are unusually bad (or good) to focus the investigation.

**2) Define “activation” and measure time-to-value**
- Pick an activation event that represents real value (not “signed up”): e.g., “created first project + invited teammate + received outcome”.
- Measure: activation rate, median time-to-activation, and where people drop before it.
- Compare “retained” vs “churned” users: what did retained users do in their first session that churned users didn’t?

**3) Map the first-run journey and find the leak**
- Create a simple funnel from landing → signup → onboarding steps → first successful outcome → second session trigger.
- For the biggest drop-off step, use session replays/heatmaps/support tickets (or lightweight screen recordings) to see the exact friction (confusing copy, missing defaults, slow load, permissions, errors).

**4) Check for audience/expectation mismatch**
- Review top ads/SEO pages and onboarding messaging: are you promising something different than the product delivers in the first 5–10 minutes?
- Look for “tourists”: high signups from broad channels with low intent; they can swamp retention and hide product issues.

**5) Add qualitative truth fast (10–15 conversations)**
- Interview: 5 retained users + 10 who churned after first use (within 48–72 hours if possible).
- Ask: “What did you come to do?”, “Where did you get stuck?”, “What did you do instead?”, “What would make this a must-return?”  
- You’re hunting for one of: missing core feature, unclear next step, trust concern, setup burden, wrong target user.

**6) Turn findings into tight experiments**
- If value is unclear: shorten onboarding, remove steps, add templates/defaults, make the “aha” visible, show progress.
- If reason-to-return is missing: create a natural cadence (reminders, reports, alerts, scheduled value, collaboration hooks) *only after* activation works.
- If reliability/performance is the culprit: instrument errors, slow endpoints, and failed actions; fix those before polishing onboarding.

If you share (1) your current `D1/D7` retention, (2) what “successful first use” looks like, and (3) your top 2 acquisition channels, I can help you define an activation metric and the exact cohort/funnel cuts to run.
```


### application

Prompt:

```
Help me apply the Hierarchy of Engagement to our note-taking app. What should our core action be, and how do we build accruing benefits around it?
```

Response:

```
**Core action**
- Make the core action “capture a note you can reliably reuse later” (capture + just enough context to be findable). The promise isn’t writing; it’s future recall in seconds.

**Hierarchy of engagement (ladder)**
- Capture (fastest path to value) → Retrieve (prove it’s trustworthy) → Reuse (apply in real work) → Synthesize (turn many notes into artifacts) → Share/Collaborate (optional, highest effort).

**Accruing benefits (how it compounds)**
- Automatic context: add time/project/people/source by default; suggest titles/tags/links so reuse gets easier without extra work.
- Retrieval surfaces: “instant search” plus proactive resurfacing (meeting start shows last notes with attendees; project page shows recent decisions; daily/weekly recap).
- Reuse mechanics: one-click “insert into doc”, “turn into task”, “save as template”, “quote/cite source” so notes directly reduce work elsewhere.
- Synthesis: topic/project hubs that auto-aggregate linked notes + summaries, so the library becomes a knowledge base rather than an archive.
- Reinforcing loop to design for: more captured context → better resurfacing + search → more successful reuse → higher trust → more capture. Track “time-to-retrieve” and “reuse rate” as the compounding-value metrics.
```


