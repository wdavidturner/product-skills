# Skill Eval Feedback

skill: okrs
agent: codex
model: gpt-5.2
date: 2026-01-15
prompt-set: evals/prompts/skill-prompts.yaml

## Prompt Results

| Prompt ID | Triggered? | Helpful? (1-5) | Too Much Context? | Missing Context? | Notes |
|-----------|------------|----------------|-------------------|------------------|-------|
| minimal |  |  |  |  |  |
| context |  |  |  |  |  |

## Overall

- Trigger quality:
- Usefulness:
- Over/under-trigger risk:
- Suggested description tweaks:
- Suggested SKILL.md tweaks:

## Raw Output

### minimal

Prompt:

```
How do I write good OKRs? What makes them effective vs. useless?
```

Response:

```
**What Good OKRs Look Like**
- **Objective**: a clear, motivating *outcome* you want (plain language, not a solution).
- **Key Results**: 3–5 **measurable outcomes** that prove the objective happened (metrics with baseline → target, by a date).
- **Scope**: one team’s controllable surface area; explicit owner; explicit timeframe.

**What Makes Them Effective**
- **Outcome over output**: “Increase activation from 22% → 30%” (outcome) vs “Ship onboarding v2” (output).
- **Specific and falsifiable**: a stranger can tell if you hit it without debate.
- **Right difficulty**: mix of **commit** (must-hit) and **stretch** (aspirational), stated explicitly.
- **Balanced signals**: include both growth and quality/health (e.g., conversion *and* retention/support burden).
- **Reviewed regularly**: weekly check-in on leading indicators; end-of-cycle scoring and learnings.

**What Makes Them Useless (Common Anti‑Patterns)**
- **Task lists** (“Launch X”, “Implement Y”) with no measurable impact.
- **Vanity metrics** (page views, “engagement”) without a defined behavior/value.
- **Too many** (10+ KRs) or too broad (“Be the best platform”) to drive choices.
- **No baseline** (you can’t tell if the target is meaningful) or no owner/cadence.

**Simple Template**
- **Objective**: *Improve [customer/business outcome] for [who] in [timeframe].*
- **KR1–KR3**: *Move [metric] from [baseline] to [target] by [date], measured by [source].*

If you tell me your team/function (product, eng, GTM, etc.) and the problem you’re tackling this quarter, I can help rewrite your draft OKRs into outcome-based ones.

[stderr]
OpenAI Codex v0.80.0 (research preview)
--------
workdir: /Users/davidturner/Code/Playground/ProductSkills
model: gpt-5.2
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019bc21a-c7c1-7070-84ea-b1d9585623b7
--------
user
How do I write good OKRs? What makes them effective vs. useless?
mcp startup: no servers
codex
**What Good OKRs Look Like**
- **Objective**: a clear, motivating *outcome* you want (plain language, not a solution).
- **Key Results**: 3–5 **measurable outcomes** that prove the objective happened (metrics with baseline → target, by a date).
- **Scope**: one team’s controllable surface area; explicit owner; explicit timeframe.

**What Makes Them Effective**
- **Outcome over output**: “Increase activation from 22% → 30%” (outcome) vs “Ship onboarding v2” (output).
- **Specific and falsifiable**: a stranger can tell if you hit it without debate.
- **Right difficulty**: mix of **commit** (must-hit) and **stretch** (aspirational), stated explicitly.
- **Balanced signals**: include both growth and quality/health (e.g., conversion *and* retention/support burden).
- **Reviewed regularly**: weekly check-in on leading indicators; end-of-cycle scoring and learnings.

**What Makes Them Useless (Common Anti‑Patterns)**
- **Task lists** (“Launch X”, “Implement Y”) with no measurable impact.
- **Vanity metrics** (page views, “engagement”) without a defined behavior/value.
- **Too many** (10+ KRs) or too broad (“Be the best platform”) to drive choices.
- **No baseline** (you can’t tell if the target is meaningful) or no owner/cadence.

**Simple Template**
- **Objective**: *Improve [customer/business outcome] for [who] in [timeframe].*
- **KR1–KR3**: *Move [metric] from [baseline] to [target] by [date], measured by [source].*

If you tell me your team/function (product, eng, GTM, etc.) and the problem you’re tackling this quarter, I can help rewrite your draft OKRs into outcome-based ones.
tokens used
1,453
```


### scenario

Prompt:

```
Our team sets OKRs every quarter but they never feel meaningful. We either hit them easily or ignore them. How do we make OKRs actually drive behavior?
```

Response:

```
OKRs drive behavior only when they force tradeoffs, are reviewed like a dashboard, and are written as outcomes with clear ownership.

**Make them meaningful**
- Start from a single quarterly “bet”: what must change by end of quarter, and what will you *stop doing* to make room.
- Write Objectives as a short outcome narrative (not a project): “Improve X for Y segment” not “Launch feature Z”.
- Make KRs measurable outcomes with baseline → target → date, and an owner: “Activation 22% → 30% by Mar 31 (Owner: …)”.
- Calibrate difficulty explicitly: label KRs as **Committed** (must hit) vs **Aspirational** (stretch; 60–70% is okay). If you “hit easily,” you set output KRs or under-shot targets.
- Limit volume: 1–2 Objectives per team, 2–4 KRs each. Too many = none matter.

**Make them drive weekly behavior**
- Run a 20–30 min weekly OKR review: KR trend, confidence (green/yellow/red), top 1–2 next actions, and a decision (double-down / change approach / de-scope).
- Prefer leading indicators where possible (things that move before the lagging metric), so teams know what to do this week.
- Tie work to KRs in planning: every major initiative must map to a KR *or it doesn’t ship this quarter*.

**Quick litmus tests (if “no,” rewrite)**
- Would two people measure the KR the same way?
- If the KR turns red, do you know what you’d change next week?
- Does hitting the KR require saying “no” to something else?

If you share one real OKR set (Objective + KRs), I can rewrite it into outcome-based KRs and suggest a weekly review cadence that fits your team.

[stderr]
OpenAI Codex v0.80.0 (research preview)
--------
workdir: /Users/davidturner/Code/Playground/ProductSkills
model: gpt-5.2
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019bc21a-f8f2-72f0-9835-0cc62e4074e9
--------
user
Our team sets OKRs every quarter but they never feel meaningful. We either hit them easily or ignore them. How do we make OKRs actually drive behavior?
mcp startup: no servers


thinking
**Providing OKR Guidance**

I see the user is asking for non-coding advice regarding OKRs, and I don't need any tools for this. While there isn’t a direct skill that matches, I can offer a practical framework. I should emphasize choosing outcomes over outputs, setting a roofshot, baseline, and target, and ensuring key results are measurable. I'll outline anti-patterns, like too many objectives and lack of accountability, and provide a concise workshop agenda. I'll include both examples of good and bad OKRs as well.
thinking
**Fixing OKRs**

I'm looking to craft some fixes for OKRs. First, I should emphasize strategy mapping and picking one or two priorities. Establishing weekly check-ins and scoring will be essential. I’ll suggest starting with behavior change: asking what we’ll do differently each week. Next, objectives should tell a story, while key results need specific metrics with baselines and owners. Limiting OKRs to 1-3 per team and 3-5 KRs per objective is key. I should also focus on making tradeoffs explicit and using learning from resets.
codex
OKRs drive behavior only when they force tradeoffs, are reviewed like a dashboard, and are written as outcomes with clear ownership.

**Make them meaningful**
- Start from a single quarterly “bet”: what must change by end of quarter, and what will you *stop doing* to make room.
- Write Objectives as a short outcome narrative (not a project): “Improve X for Y segment” not “Launch feature Z”.
- Make KRs measurable outcomes with baseline → target → date, and an owner: “Activation 22% → 30% by Mar 31 (Owner: …)”.
- Calibrate difficulty explicitly: label KRs as **Committed** (must hit) vs **Aspirational** (stretch; 60–70% is okay). If you “hit easily,” you set output KRs or under-shot targets.
- Limit volume: 1–2 Objectives per team, 2–4 KRs each. Too many = none matter.

**Make them drive weekly behavior**
- Run a 20–30 min weekly OKR review: KR trend, confidence (green/yellow/red), top 1–2 next actions, and a decision (double-down / change approach / de-scope).
- Prefer leading indicators where possible (things that move before the lagging metric), so teams know what to do this week.
- Tie work to KRs in planning: every major initiative must map to a KR *or it doesn’t ship this quarter*.

**Quick litmus tests (if “no,” rewrite)**
- Would two people measure the KR the same way?
- If the KR turns red, do you know what you’d change next week?
- Does hitting the KR require saying “no” to something else?

If you share one real OKR set (Objective + KRs), I can rewrite it into outcome-based KRs and suggest a weekly review cadence that fits your team.
tokens used
1,730
```


### application

Prompt:

```
Help me write OKRs for our growth team this quarter. Our focus is improving activation rate from 20% to 35%. What should the objective and key results look like?
```

Response:

```
**Objective**  
Deliver a clearer, faster first-value experience that increases new-user activation from 20% to 35% this quarter.

**Key Results**
- Increase activation rate (weekly cohort; define “activated” as `X` within `Y` days) from 20% → 35%.
- Improve conversion on the top two drop-off steps in the activation funnel (e.g., `Signup→Onboarding complete`, `Onboarding complete→First key action`) by +10pp each (or to agreed targets).
- Reduce median time-to-activation from current baseline → same-day activation (and reduce p90 by 30%).
- Ship 3+ activation improvements proven via experiments, each delivering ≥ +3pp absolute activation lift (or equivalent weighted impact).
- Run ≥12 activation experiments this quarter with ≥80% meeting pre-defined quality thresholds (power, instrumentation, decision rules).
- Guardrail: maintain D7 retention within 5% of baseline and keep activation-related support tickets/defects from increasing week-over-week.

[stderr]
OpenAI Codex v0.80.0 (research preview)
--------
workdir: /Users/davidturner/Code/Playground/ProductSkills
model: gpt-5.2
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019bc21b-427f-7e10-b9b5-f84a2c4c8ec7
--------
user
Help me write OKRs for our growth team this quarter. Our focus is improving activation rate from 20% to 35%. What should the objective and key results look like?
mcp startup: no servers



thinking
**Drafting OKRs for Growth Team**

I'm focusing on writing OKRs for the growth team this quarter, specifically aiming to boost the activation rate from 20% to 35%. The skills lists aren’t relevant for this task, so I'll keep it straightforward. I'll suggest a concise objective and key results, outlining leading indicators and metrics such as onboarding completion and cohort retention. It's crucial to define the activation metric clearly, proposing it as KR0. Overall, I need to keep the detail to around six bullets.
thinking
**Creating OKRs for Activation Improvement**

I need to develop a cohesive OKR set with one objective and four to six key results, including baseline and target metrics. Here’s what I’m thinking:

Objective: "Deliver a faster, clearer first-value experience to increase activation from 20% to 35% for new signups by the end of the quarter." 

The key results could be:  
1) Raise the activation rate from 20% to 35%.  
2) Improve conversion rates at two key steps.  
3) Reduce the median time-to-activation.  
4) Implement three validated onboarding changes.  
5) Conduct 12 experiments to meet quality standards.  

For guardrails, I'll maintain D7 retention and monitor support tickets.
thinking
**Refining OKRs for Activation Metrics**

I want to ensure the user gets practical OKRs, so I should avoid placeholders and instead specify traditional metrics, like achieving time-to-first-value within the same day. 

To keep it clear, let's set an objective with five to six key results that include baselines as variables. I could propose including “Finalize activation definition and instrument funnel by week 2” as part of a key result. 

So, I’ll draft one objective and six key results, focusing on measurable metrics and including guardrails. I don’t need to ask about the product type here!
codex
**Objective**  
Deliver a clearer, faster first-value experience that increases new-user activation from 20% to 35% this quarter.

**Key Results**
- Increase activation rate (weekly cohort; define “activated” as `X` within `Y` days) from 20% → 35%.
- Improve conversion on the top two drop-off steps in the activation funnel (e.g., `Signup→Onboarding complete`, `Onboarding complete→First key action`) by +10pp each (or to agreed targets).
- Reduce median time-to-activation from current baseline → same-day activation (and reduce p90 by 30%).
- Ship 3+ activation improvements proven via experiments, each delivering ≥ +3pp absolute activation lift (or equivalent weighted impact).
- Run ≥12 activation experiments this quarter with ≥80% meeting pre-defined quality thresholds (power, instrumentation, decision rules).
- Guardrail: maintain D7 retention within 5% of baseline and keep activation-related support tickets/defects from increasing week-over-week.
tokens used
1,676
```


